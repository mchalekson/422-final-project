{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd95e218",
   "metadata": {},
   "source": [
    "# YouTube Pipeline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff25c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: ID parsing + batching\n",
    "# -----------------------------\n",
    "\n",
    "_YT_ID_RE = re.compile(r\"(?:v=|\\/shorts\\/|youtu\\.be\\/|\\/embed\\/)([A-Za-z0-9_-]{11})\")\n",
    "\n",
    "def extract_video_id(url: str) -> Optional[str]:\n",
    "    \"\"\"Extract 11-char YouTube video_id from common URL formats.\"\"\"\n",
    "    if not isinstance(url, str) or not url.strip():\n",
    "        return None\n",
    "    m = _YT_ID_RE.search(url)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def chunked(lst: List[str], n: int) -> List[List[str]]:\n",
    "    return [lst[i:i+n] for i in range(0, len(lst), n)]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# YouTube Data API client (requests-based)\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class YouTubeAPI:\n",
    "    api_key: str\n",
    "    base_url: str = \"https://www.googleapis.com/youtube/v3\"\n",
    "    session: Optional[requests.Session] = None\n",
    "    sleep_s: float = 0.1  # tiny throttle to be polite / avoid rate spikes\n",
    "\n",
    "    def _sess(self) -> requests.Session:\n",
    "        if self.session is None:\n",
    "            self.session = requests.Session()\n",
    "        return self.session\n",
    "\n",
    "    def _get(self, path: str, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        params = dict(params)\n",
    "        params[\"key\"] = self.api_key\n",
    "        url = f\"{self.base_url}/{path}\"\n",
    "        r = self._sess().get(url, params=params, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            raise RuntimeError(f\"YT API error {r.status_code}: {r.text[:500]}\")\n",
    "        time.sleep(self.sleep_s)\n",
    "        return r.json()\n",
    "\n",
    "    def videos_list(self, video_ids: List[str], parts: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Fetch video resources for up to 50 ids.\"\"\"\n",
    "        out = []\n",
    "        for batch in chunked(video_ids, 50):\n",
    "            data = self._get(\"videos\", {\"part\": parts, \"id\": \",\".join(batch), \"maxResults\": 50})\n",
    "            out.extend(data.get(\"items\", []))\n",
    "        return out\n",
    "\n",
    "    def channels_list(self, channel_ids: List[str], parts: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Fetch channel resources for up to 50 ids.\"\"\"\n",
    "        out = []\n",
    "        for batch in chunked(channel_ids, 50):\n",
    "            data = self._get(\"channels\", {\"part\": parts, \"id\": \",\".join(batch), \"maxResults\": 50})\n",
    "            out.extend(data.get(\"items\", []))\n",
    "        return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Light feature extraction\n",
    "# -----------------------------\n",
    "\n",
    "def iso8601_duration_to_seconds(dur: Optional[str]) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Convert ISO 8601 duration like PT1H2M3S to seconds.\n",
    "    \"\"\"\n",
    "    if not dur or not isinstance(dur, str):\n",
    "        return None\n",
    "    # Simple parser: PT#H#M#S\n",
    "    h = m = s = 0\n",
    "    mobj = re.match(r\"^PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?$\", dur)\n",
    "    if not mobj:\n",
    "        return None\n",
    "    if mobj.group(1): h = int(mobj.group(1))\n",
    "    if mobj.group(2): m = int(mobj.group(2))\n",
    "    if mobj.group(3): s = int(mobj.group(3))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "\n",
    "def pick_best_thumbnail(thumbnails: Dict[str, Any]) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Choose best available thumbnail URL from snippet.thumbnails.\n",
    "    Returns (url, label).\n",
    "    \"\"\"\n",
    "    if not isinstance(thumbnails, dict):\n",
    "        return (None, None)\n",
    "    order = [\"maxres\", \"standard\", \"high\", \"medium\", \"default\"]\n",
    "    for k in order:\n",
    "        if k in thumbnails and isinstance(thumbnails[k], dict) and \"url\" in thumbnails[k]:\n",
    "            return thumbnails[k][\"url\"], k\n",
    "    return (None, None)\n",
    "\n",
    "\n",
    "# Optional image features (requires pillow + numpy)\n",
    "def compute_image_features_from_url(img_url: str, cache_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Download image to cache_dir and compute basic features:\n",
    "    - width, height\n",
    "    - mean_brightness (0-255)\n",
    "    - colorfulness (Hasler & Süsstrunk-ish approximation)\n",
    "    - file_hash (sha1)\n",
    "    \"\"\"\n",
    "    feats: Dict[str, Any] = {\n",
    "        \"thumb_path\": None,\n",
    "        \"thumb_sha1\": None,\n",
    "        \"thumb_width\": None,\n",
    "        \"thumb_height\": None,\n",
    "        \"thumb_mean_brightness\": None,\n",
    "        \"thumb_colorfulness\": None,\n",
    "    }\n",
    "    if not img_url:\n",
    "        return feats\n",
    "\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    # deterministic filename from URL\n",
    "    fname = hashlib.sha1(img_url.encode(\"utf-8\")).hexdigest() + \".jpg\"\n",
    "    fpath = os.path.join(cache_dir, fname)\n",
    "\n",
    "    # download if missing\n",
    "    if not os.path.exists(fpath):\n",
    "        r = requests.get(img_url, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return feats\n",
    "        with open(fpath, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "    feats[\"thumb_path\"] = fpath\n",
    "\n",
    "    # compute features\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        import numpy as np\n",
    "\n",
    "        with open(fpath, \"rb\") as f:\n",
    "            b = f.read()\n",
    "        feats[\"thumb_sha1\"] = hashlib.sha1(b).hexdigest()\n",
    "\n",
    "        img = Image.open(fpath).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        feats[\"thumb_width\"] = w\n",
    "        feats[\"thumb_height\"] = h\n",
    "\n",
    "        arr = np.asarray(img).astype(np.float32)  # (H,W,3)\n",
    "        # brightness (simple luminance proxy)\n",
    "        brightness = 0.2126 * arr[..., 0] + 0.7152 * arr[..., 1] + 0.0722 * arr[..., 2]\n",
    "        feats[\"thumb_mean_brightness\"] = float(np.mean(brightness))\n",
    "\n",
    "        # colorfulness approximation (Hasler & Süsstrunk style)\n",
    "        rg = arr[..., 0] - arr[..., 1]\n",
    "        yb = 0.5 * (arr[..., 0] + arr[..., 1]) - arr[..., 2]\n",
    "        std_rg = float(np.std(rg))\n",
    "        std_yb = float(np.std(yb))\n",
    "        mean_rg = float(np.mean(rg))\n",
    "        mean_yb = float(np.mean(yb))\n",
    "        feats[\"thumb_colorfulness\"] = float(math.sqrt(std_rg**2 + std_yb**2) + 0.3 * math.sqrt(mean_rg**2 + mean_yb**2))\n",
    "\n",
    "    except Exception:\n",
    "        # If PIL/numpy not installed or image parse fails, just skip features.\n",
    "        pass\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main function: enrich CSV -> add columns\n",
    "# -----------------------------\n",
    "\n",
    "def enrich_youtube_csv(\n",
    "    csv_path: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    video_id_col: str = \"video_id\",\n",
    "    url_col: str = \"url\",\n",
    "    out_csv_path: Optional[str] = None,\n",
    "    add_thumbnail_features: bool = False,\n",
    "    thumbnail_cache_dir: str = \"thumb_cache\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enrich a CSV containing YouTube video IDs and/or URLs.\n",
    "\n",
    "    Adds columns:\n",
    "      Video:\n",
    "        - yt_title, yt_description, yt_published_at, yt_duration_sec\n",
    "        - yt_category_id, yt_tags_json, yt_default_language, yt_default_audio_language\n",
    "        - yt_made_for_kids, yt_live_broadcast_content\n",
    "        - yt_view_count, yt_like_count, yt_comment_count\n",
    "        - yt_channel_id\n",
    "        - yt_thumb_url, yt_thumb_quality\n",
    "      Channel:\n",
    "        - yt_channel_title, yt_channel_published_at, yt_channel_country\n",
    "        - yt_subscriber_count, yt_channel_view_count, yt_channel_video_count\n",
    "    Optionally adds thumbnail image features if add_thumbnail_features=True.\n",
    "\n",
    "    Requirements:\n",
    "      - requests, pandas\n",
    "      - optional: pillow, numpy for thumbnail features\n",
    "\n",
    "    Notes:\n",
    "      - YouTube API quota applies.\n",
    "      - Missing/removed videos will simply not return items; those rows remain NaN.\n",
    "    \"\"\"\n",
    "    api_key = api_key or os.getenv(\"YOUTUBE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Missing YouTube API key. Pass api_key=... or set YOUTUBE_API_KEY env var.\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Ensure video_id exists: use explicit column or parse from URL\n",
    "    if video_id_col not in df.columns:\n",
    "        df[video_id_col] = None\n",
    "\n",
    "    if url_col in df.columns:\n",
    "        missing_vid = df[video_id_col].isna() | (df[video_id_col].astype(str).str.strip() == \"\")\n",
    "        df.loc[missing_vid, video_id_col] = df.loc[missing_vid, url_col].apply(extract_video_id)\n",
    "\n",
    "    # Clean ids\n",
    "    df[video_id_col] = df[video_id_col].astype(str).str.strip()\n",
    "    df.loc[df[video_id_col].isin([\"\", \"nan\", \"None\"]), video_id_col] = pd.NA\n",
    "\n",
    "    video_ids = df[video_id_col].dropna().unique().tolist()\n",
    "    if not video_ids:\n",
    "        raise ValueError(\"No valid video IDs found in the CSV (either in video_id_col or parsed from url_col).\")\n",
    "\n",
    "    yt = YouTubeAPI(api_key=api_key)\n",
    "\n",
    "    # --- Fetch video data ---\n",
    "    video_parts = \"snippet,contentDetails,statistics,status\"\n",
    "    video_items = yt.videos_list(video_ids, parts=video_parts)\n",
    "\n",
    "    video_rows: Dict[str, Dict[str, Any]] = {}\n",
    "    channel_ids: List[str] = []\n",
    "\n",
    "    for item in video_items:\n",
    "        vid = item.get(\"id\")\n",
    "        snippet = item.get(\"snippet\", {}) or {}\n",
    "        stats = item.get(\"statistics\", {}) or {}\n",
    "        content = item.get(\"contentDetails\", {}) or {}\n",
    "        status = item.get(\"status\", {}) or {}\n",
    "\n",
    "        channel_id = snippet.get(\"channelId\")\n",
    "        if channel_id:\n",
    "            channel_ids.append(channel_id)\n",
    "\n",
    "        thumb_url, thumb_quality = pick_best_thumbnail(snippet.get(\"thumbnails\", {}))\n",
    "\n",
    "        video_rows[vid] = {\n",
    "            \"yt_title\": snippet.get(\"title\"),\n",
    "            \"yt_description\": snippet.get(\"description\"),\n",
    "            \"yt_published_at\": snippet.get(\"publishedAt\"),\n",
    "            \"yt_duration_sec\": iso8601_duration_to_seconds(content.get(\"duration\")),\n",
    "            \"yt_category_id\": snippet.get(\"categoryId\"),\n",
    "            \"yt_tags_json\": json.dumps(snippet.get(\"tags\")) if snippet.get(\"tags\") is not None else None,\n",
    "            \"yt_default_language\": snippet.get(\"defaultLanguage\"),\n",
    "            \"yt_default_audio_language\": snippet.get(\"defaultAudioLanguage\"),\n",
    "            \"yt_made_for_kids\": status.get(\"madeForKids\"),\n",
    "            \"yt_live_broadcast_content\": snippet.get(\"liveBroadcastContent\"),\n",
    "            \"yt_view_count\": safe_int(stats.get(\"viewCount\")),\n",
    "            \"yt_like_count\": safe_int(stats.get(\"likeCount\")),\n",
    "            \"yt_comment_count\": safe_int(stats.get(\"commentCount\")),\n",
    "            \"yt_channel_id\": channel_id,\n",
    "            \"yt_thumb_url\": thumb_url,\n",
    "            \"yt_thumb_quality\": thumb_quality,\n",
    "        }\n",
    "\n",
    "    # Map video enrichment back to df\n",
    "    video_enriched = pd.DataFrame.from_dict(video_rows, orient=\"index\")\n",
    "    video_enriched.index.name = video_id_col\n",
    "    df = df.merge(video_enriched, how=\"left\", left_on=video_id_col, right_index=True)\n",
    "\n",
    "    # --- Fetch channel data ---\n",
    "    channel_ids = sorted(set([c for c in df[\"yt_channel_id\"].dropna().unique().tolist() if isinstance(c, str)]))\n",
    "    if channel_ids:\n",
    "        channel_parts = \"snippet,statistics\"\n",
    "        channel_items = yt.channels_list(channel_ids, parts=channel_parts)\n",
    "\n",
    "        channel_rows: Dict[str, Dict[str, Any]] = {}\n",
    "        for item in channel_items:\n",
    "            cid = item.get(\"id\")\n",
    "            snippet = item.get(\"snippet\", {}) or {}\n",
    "            stats = item.get(\"statistics\", {}) or {}\n",
    "            channel_rows[cid] = {\n",
    "                \"yt_channel_title\": snippet.get(\"title\"),\n",
    "                \"yt_channel_published_at\": snippet.get(\"publishedAt\"),\n",
    "                \"yt_channel_country\": snippet.get(\"country\"),\n",
    "                \"yt_subscriber_count\": safe_int(stats.get(\"subscriberCount\")),\n",
    "                \"yt_channel_view_count\": safe_int(stats.get(\"viewCount\")),\n",
    "                \"yt_channel_video_count\": safe_int(stats.get(\"videoCount\")),\n",
    "            }\n",
    "\n",
    "        channel_enriched = pd.DataFrame.from_dict(channel_rows, orient=\"index\")\n",
    "        channel_enriched.index.name = \"yt_channel_id\"\n",
    "        df = df.merge(channel_enriched, how=\"left\", on=\"yt_channel_id\")\n",
    "\n",
    "    # --- Optional: thumbnail image features ---\n",
    "    if add_thumbnail_features:\n",
    "        feats_list = []\n",
    "        for _, row in df.iterrows():\n",
    "            feats = compute_image_features_from_url(row.get(\"yt_thumb_url\"), thumbnail_cache_dir)\n",
    "            feats_list.append(feats)\n",
    "        feats_df = pd.DataFrame(feats_list)\n",
    "        df = pd.concat([df.reset_index(drop=True), feats_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Save\n",
    "    if out_csv_path:\n",
    "        df.to_csv(out_csv_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def safe_int(x: Any) -> Optional[int]:\n",
    "    try:\n",
    "        if x is None:\n",
    "            return None\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffcd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched = enrich_youtube_csv(\n",
    "    csv_path='/Users/maxchalekson/Northwestern University/Winter-2026/MSDS-422-0/Final-Project/422-final-project/youtube_data.csv',\n",
    "    api_key=os.getenv(\"YOUTUBE_API_KEY\"),   # or paste your key here (not recommended)\n",
    "    video_id_col=\"video_id\",\n",
    "    url_col=\"url\",\n",
    "    out_csv_path=\"/mnt/data/youtube_data_enriched.csv\",\n",
    "    add_thumbnail_features=True,            # set False if you don’t want downloads/features\n",
    "    thumbnail_cache_dir=\"/mnt/data/thumb_cache\",\n",
    ")\n",
    "\n",
    "print(df_enriched.shape)\n",
    "print(df_enriched.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "430-0-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
