{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd95e218",
   "metadata": {},
   "source": [
    "# YouTube Pipeline Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ba570",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook constructs a large-scale, analysis-ready dataset for studying YouTube virality by enriching a raw list of video URLs/IDs with creator-level, content-level, and engagement metadata from the YouTube Data API. The resulting dataset integrates textual metadata (titles, descriptions, tags), temporal features (video age, duration), creator social capital (subscriber counts, channel history), and engagement signals (views, likes, comments), enabling systematic investigation of which attributes are most strongly associated with viral outcomes.\n",
    "\n",
    "## Feature scope and design choices\n",
    "\n",
    "In addition to standard metadata, the pipeline optionally extracts lightweight thumbnail image features (e.g., brightness, colorfulness, resolution) as coarse proxies for visual salience. These features are intentionally limited in complexity and are used only as exploratory controls rather than deep semantic representations. Raw thumbnail images are cached temporarily during processing and discarded after feature extraction to maintain a compact, reproducible dataset.\n",
    "\n",
    "## Methodological motivation\n",
    "\n",
    "Consistent with prior work, this notebook prioritizes features that are observable at or near publication time (creator characteristics, metadata, visual proxies) while also retaining post-publication engagement metrics for comparative and explanatory analyses. This design reflects the central research objective: distinguishing predictors that plausibly contribute to virality from signals that primarily emerge after diffusion has already occurred.\n",
    "\n",
    "## Output dataset\n",
    "\n",
    "The final output is a single enriched CSV file containing one row per YouTube video, identified by `video_id` (from the **youtube_data.csv**), and integrating technical, semantic, social, and engagement-related features. The dataset includes raw video and encoding attributes such as duration, resolution, bitrate, frame rate, and codec, alongside scraped metadata including category, URL, title, description, hashtags, and observed view, like, and comment counts. These fields are augmented with structured YouTube Data API features capturing publication timing (`yt_published_at`), standardized duration (`yt_duration_sec`), content category (`yt_category_id`), language metadata, live-broadcast and audience-designation flags, and validated engagement metrics (`yt_view_count`, `yt_like_count`, `yt_comment_count`). Creator-level social capital is represented through channel identifiers and attributes, including channel age, country, subscriber count, total channel views, and upload volume. Finally, the dataset optionally includes lightweight, interpretable thumbnail-derived visual proxies—such as resolution, mean brightness, and colorfulness—computed from thumbnail URLs without retaining raw image data. Together, these columns produce an analysis-ready table that supports exploratory analysis, statistical modeling, and machine learning approaches for examining the relative influence of content design, creator reputation, and engagement dynamics on YouTube virality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7190af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists? True\n",
      "dotenv loaded? True\n",
      "Kernel sees key? True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "ENV_PATH = \"/Users/maxchalekson/Northwestern University/Winter-2026/MSDS-422-0/Final-Project/422-final-project/.env\"\n",
    "\n",
    "print(\"File exists?\", os.path.exists(ENV_PATH))\n",
    "loaded = load_dotenv(ENV_PATH, override=True)\n",
    "print(\"dotenv loaded?\", loaded)\n",
    "print(\"Kernel sees key?\", bool(os.getenv(\"YOUTUBE_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91b115c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching video metadata: 100%|██████████| 352/352 [02:01<00:00,  2.90batch/s]\n",
      "Fetching channel metadata: 100%|██████████| 347/347 [01:18<00:00,  4.43batch/s]\n",
      "Processing unique thumbnails: 100%|██████████| 17491/17491 [1:05:37<00:00,  4.44image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17589, 45)\n",
      "['video_id', 'duration', 'bitrate', 'bitrate(video)', 'height', 'width', 'frame rate', 'frame rate(est.)', 'codec', 'category', 'url', 'title', 'description', 'hashtags', 'views', 'likes', 'comments', 'yt_title', 'yt_description', 'yt_published_at', 'yt_duration_sec', 'yt_category_id', 'yt_tags_json', 'yt_default_language', 'yt_default_audio_language', 'yt_made_for_kids', 'yt_live_broadcast_content', 'yt_view_count', 'yt_like_count', 'yt_comment_count', 'yt_channel_id', 'yt_thumb_url', 'yt_thumb_quality', 'yt_channel_title', 'yt_channel_published_at', 'yt_channel_country', 'yt_subscriber_count', 'yt_channel_view_count', 'yt_channel_video_count', 'thumb_path', 'thumb_sha1', 'thumb_width', 'thumb_height', 'thumb_mean_brightness', 'thumb_colorfulness']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm  # progress bars for notebook + terminal\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Project config (edit these)\n",
    "# -----------------------------\n",
    "\n",
    "PROJECT_DIR = \"/Users/maxchalekson/Northwestern University/Winter-2026/MSDS-422-0/Final-Project/422-final-project\"\n",
    "ENV_PATH = os.path.join(PROJECT_DIR, \".env\")  # should contain: YOUTUBE_API_KEY=...\n",
    "\n",
    "\n",
    "def ensure_youtube_api_key(env_path: str = ENV_PATH) -> str:\n",
    "    key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "    if key:\n",
    "        return key\n",
    "\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv(env_path, override=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "    if key:\n",
    "        return key\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Missing YouTube API key.\\n\"\n",
    "        \"Fix:\\n\"\n",
    "        f\"  1) Create {env_path}\\n\"\n",
    "        \"     with a line like: YOUTUBE_API_KEY=AIzaSy...\\n\"\n",
    "        \"  2) Run: pip install python-dotenv\\n\"\n",
    "        \"  3) Restart your Jupyter kernel / VS Code window\\n\"\n",
    "        \"Also ensure .env is in .gitignore so you don't leak your key.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: ID parsing + batching\n",
    "# -----------------------------\n",
    "\n",
    "_YT_ID_RE = re.compile(r\"(?:v=|\\/shorts\\/|youtu\\.be\\/|\\/embed\\/)([A-Za-z0-9_-]{11})\")\n",
    "\n",
    "\n",
    "def extract_video_id(url: str) -> Optional[str]:\n",
    "    if not isinstance(url, str) or not url.strip():\n",
    "        return None\n",
    "    m = _YT_ID_RE.search(url)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def chunked(lst: List[str], n: int) -> List[List[str]]:\n",
    "    return [lst[i : i + n] for i in range(0, len(lst), n)]\n",
    "\n",
    "\n",
    "def safe_int(x: Any) -> Optional[int]:\n",
    "    try:\n",
    "        if x is None:\n",
    "            return None\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# YouTube Data API client\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class YouTubeAPI:\n",
    "    api_key: str\n",
    "    base_url: str = \"https://www.googleapis.com/youtube/v3\"\n",
    "    session: Optional[requests.Session] = None\n",
    "    sleep_s: float = 0.1\n",
    "\n",
    "    def _sess(self) -> requests.Session:\n",
    "        if self.session is None:\n",
    "            self.session = requests.Session()\n",
    "        return self.session\n",
    "\n",
    "    def _get(self, path: str, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        params = dict(params)\n",
    "        params[\"key\"] = self.api_key\n",
    "        url = f\"{self.base_url}/{path}\"\n",
    "        r = self._sess().get(url, params=params, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            raise RuntimeError(f\"YT API error {r.status_code}: {r.text[:500]}\")\n",
    "        time.sleep(self.sleep_s)\n",
    "        return r.json()\n",
    "\n",
    "    def videos_list(self, video_ids: List[str], parts: str) -> List[Dict[str, Any]]:\n",
    "        out: List[Dict[str, Any]] = []\n",
    "        batches = chunked(video_ids, 50)\n",
    "        for batch in tqdm(batches, desc=\"Fetching video metadata\", unit=\"batch\"):\n",
    "            data = self._get(\"videos\", {\"part\": parts, \"id\": \",\".join(batch), \"maxResults\": 50})\n",
    "            out.extend(data.get(\"items\", []))\n",
    "        return out\n",
    "\n",
    "    def channels_list(self, channel_ids: List[str], parts: str) -> List[Dict[str, Any]]:\n",
    "        out: List[Dict[str, Any]] = []\n",
    "        batches = chunked(channel_ids, 50)\n",
    "        for batch in tqdm(batches, desc=\"Fetching channel metadata\", unit=\"batch\"):\n",
    "            data = self._get(\"channels\", {\"part\": parts, \"id\": \",\".join(batch), \"maxResults\": 50})\n",
    "            out.extend(data.get(\"items\", []))\n",
    "        return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Feature extraction helpers\n",
    "# -----------------------------\n",
    "\n",
    "def iso8601_duration_to_seconds(dur: Optional[str]) -> Optional[int]:\n",
    "    if not dur or not isinstance(dur, str):\n",
    "        return None\n",
    "    h = m = s = 0\n",
    "    mobj = re.match(r\"^PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?$\", dur)\n",
    "    if not mobj:\n",
    "        return None\n",
    "    if mobj.group(1): h = int(mobj.group(1))\n",
    "    if mobj.group(2): m = int(mobj.group(2))\n",
    "    if mobj.group(3): s = int(mobj.group(3))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "\n",
    "def pick_best_thumbnail(thumbnails: Dict[str, Any]) -> Tuple[Optional[str], Optional[str]]:\n",
    "    if not isinstance(thumbnails, dict):\n",
    "        return (None, None)\n",
    "    order = [\"maxres\", \"standard\", \"high\", \"medium\", \"default\"]\n",
    "    for k in order:\n",
    "        if k in thumbnails and isinstance(thumbnails[k], dict) and \"url\" in thumbnails[k]:\n",
    "            return thumbnails[k][\"url\"], k\n",
    "    return (None, None)\n",
    "\n",
    "\n",
    "def compute_image_features_from_url(img_url: Any, cache_dir: str) -> Dict[str, Any]:\n",
    "    feats: Dict[str, Any] = {\n",
    "        \"thumb_path\": None,\n",
    "        \"thumb_sha1\": None,\n",
    "        \"thumb_width\": None,\n",
    "        \"thumb_height\": None,\n",
    "        \"thumb_mean_brightness\": None,\n",
    "        \"thumb_colorfulness\": None,\n",
    "    }\n",
    "\n",
    "    # handle NaN / None / non-strings\n",
    "    if not isinstance(img_url, str) or not img_url.strip():\n",
    "        return feats\n",
    "\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = hashlib.sha1(img_url.encode(\"utf-8\")).hexdigest() + \".jpg\"\n",
    "    fpath = os.path.join(cache_dir, fname)\n",
    "\n",
    "    if not os.path.exists(fpath):\n",
    "        try:\n",
    "            r = requests.get(img_url, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                return feats\n",
    "            with open(fpath, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "        except Exception:\n",
    "            return feats\n",
    "\n",
    "    feats[\"thumb_path\"] = fpath\n",
    "\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        import numpy as np\n",
    "\n",
    "        with open(fpath, \"rb\") as f:\n",
    "            b = f.read()\n",
    "        feats[\"thumb_sha1\"] = hashlib.sha1(b).hexdigest()\n",
    "\n",
    "        img = Image.open(fpath).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        feats[\"thumb_width\"] = w\n",
    "        feats[\"thumb_height\"] = h\n",
    "\n",
    "        arr = np.asarray(img).astype(np.float32)\n",
    "        brightness = 0.2126 * arr[..., 0] + 0.7152 * arr[..., 1] + 0.0722 * arr[..., 2]\n",
    "        feats[\"thumb_mean_brightness\"] = float(np.mean(brightness))\n",
    "\n",
    "        rg = arr[..., 0] - arr[..., 1]\n",
    "        yb = 0.5 * (arr[..., 0] + arr[..., 1]) - arr[..., 2]\n",
    "        std_rg = float(np.std(rg))\n",
    "        std_yb = float(np.std(yb))\n",
    "        mean_rg = float(np.mean(rg))\n",
    "        mean_yb = float(np.mean(yb))\n",
    "        feats[\"thumb_colorfulness\"] = float(\n",
    "            math.sqrt(std_rg**2 + std_yb**2) + 0.3 * math.sqrt(mean_rg**2 + mean_yb**2)\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main enrichment function\n",
    "# -----------------------------\n",
    "\n",
    "def enrich_youtube_csv(\n",
    "    csv_path: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    video_id_col: str = \"video_id\",\n",
    "    url_col: str = \"url\",\n",
    "    out_csv_path: Optional[str] = None,\n",
    "    add_thumbnail_features: bool = False,\n",
    "    thumbnail_cache_dir: str = \"thumb_cache\",\n",
    ") -> pd.DataFrame:\n",
    "    api_key = api_key or ensure_youtube_api_key()\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if video_id_col not in df.columns:\n",
    "        df[video_id_col] = pd.NA\n",
    "\n",
    "    if url_col in df.columns:\n",
    "        missing_vid = df[video_id_col].isna() | (df[video_id_col].astype(str).str.strip() == \"\")\n",
    "        df.loc[missing_vid, video_id_col] = df.loc[missing_vid, url_col].apply(extract_video_id)\n",
    "\n",
    "    df[video_id_col] = df[video_id_col].astype(str).str.strip()\n",
    "    df.loc[df[video_id_col].isin([\"\", \"nan\", \"None\"]), video_id_col] = pd.NA\n",
    "\n",
    "    video_ids = df[video_id_col].dropna().unique().tolist()\n",
    "    if not video_ids:\n",
    "        raise ValueError(\"No valid video IDs found in the CSV (either in video_id_col or parsed from url_col).\")\n",
    "\n",
    "    yt = YouTubeAPI(api_key=api_key)\n",
    "\n",
    "    # video + channel metadata\n",
    "    video_items = yt.videos_list(video_ids, parts=\"snippet,contentDetails,statistics,status\")\n",
    "    video_rows: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    for item in video_items:\n",
    "        vid = item.get(\"id\")\n",
    "        snippet = item.get(\"snippet\", {}) or {}\n",
    "        stats = item.get(\"statistics\", {}) or {}\n",
    "        content = item.get(\"contentDetails\", {}) or {}\n",
    "        status = item.get(\"status\", {}) or {}\n",
    "\n",
    "        thumb_url, thumb_quality = pick_best_thumbnail(snippet.get(\"thumbnails\", {}))\n",
    "\n",
    "        video_rows[vid] = {\n",
    "            \"yt_title\": snippet.get(\"title\"),\n",
    "            \"yt_description\": snippet.get(\"description\"),\n",
    "            \"yt_published_at\": snippet.get(\"publishedAt\"),\n",
    "            \"yt_duration_sec\": iso8601_duration_to_seconds(content.get(\"duration\")),\n",
    "            \"yt_category_id\": snippet.get(\"categoryId\"),\n",
    "            \"yt_tags_json\": json.dumps(snippet.get(\"tags\")) if snippet.get(\"tags\") is not None else None,\n",
    "            \"yt_default_language\": snippet.get(\"defaultLanguage\"),\n",
    "            \"yt_default_audio_language\": snippet.get(\"defaultAudioLanguage\"),\n",
    "            \"yt_made_for_kids\": status.get(\"madeForKids\"),\n",
    "            \"yt_live_broadcast_content\": snippet.get(\"liveBroadcastContent\"),\n",
    "            \"yt_view_count\": safe_int(stats.get(\"viewCount\")),\n",
    "            \"yt_like_count\": safe_int(stats.get(\"likeCount\")),\n",
    "            \"yt_comment_count\": safe_int(stats.get(\"commentCount\")),\n",
    "            \"yt_channel_id\": snippet.get(\"channelId\"),\n",
    "            \"yt_thumb_url\": thumb_url,\n",
    "            \"yt_thumb_quality\": thumb_quality,\n",
    "        }\n",
    "\n",
    "    video_enriched = pd.DataFrame.from_dict(video_rows, orient=\"index\")\n",
    "    video_enriched.index.name = video_id_col\n",
    "    df = df.merge(video_enriched, how=\"left\", left_on=video_id_col, right_index=True)\n",
    "\n",
    "    channel_ids = sorted(set([c for c in df[\"yt_channel_id\"].dropna().tolist() if isinstance(c, str)]))\n",
    "    if channel_ids:\n",
    "        channel_items = yt.channels_list(channel_ids, parts=\"snippet,statistics\")\n",
    "        channel_rows: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "        for item in channel_items:\n",
    "            cid = item.get(\"id\")\n",
    "            snippet = item.get(\"snippet\", {}) or {}\n",
    "            stats = item.get(\"statistics\", {}) or {}\n",
    "            channel_rows[cid] = {\n",
    "                \"yt_channel_title\": snippet.get(\"title\"),\n",
    "                \"yt_channel_published_at\": snippet.get(\"publishedAt\"),\n",
    "                \"yt_channel_country\": snippet.get(\"country\"),\n",
    "                \"yt_subscriber_count\": safe_int(stats.get(\"subscriberCount\")),\n",
    "                \"yt_channel_view_count\": safe_int(stats.get(\"viewCount\")),\n",
    "                \"yt_channel_video_count\": safe_int(stats.get(\"videoCount\")),\n",
    "            }\n",
    "\n",
    "        channel_enriched = pd.DataFrame.from_dict(channel_rows, orient=\"index\")\n",
    "        channel_enriched.index.name = \"yt_channel_id\"\n",
    "        df = df.merge(channel_enriched, how=\"left\", on=\"yt_channel_id\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # ✅ Efficient thumbnail features: unique URLs only\n",
    "    # -----------------------------\n",
    "    if add_thumbnail_features:\n",
    "        thumb_series = df[\"yt_thumb_url\"].fillna(\"\").astype(str).map(str.strip)\n",
    "\n",
    "        unique_urls = [u for u in thumb_series.unique().tolist() if u]\n",
    "        url_to_feats: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "        for url in tqdm(unique_urls, desc=\"Processing unique thumbnails\", unit=\"image\"):\n",
    "            url_to_feats[url] = compute_image_features_from_url(url, thumbnail_cache_dir)\n",
    "\n",
    "        # map back to rows (fast)\n",
    "        feats_df = thumb_series.map(lambda u: url_to_feats.get(u, {\n",
    "            \"thumb_path\": None,\n",
    "            \"thumb_sha1\": None,\n",
    "            \"thumb_width\": None,\n",
    "            \"thumb_height\": None,\n",
    "            \"thumb_mean_brightness\": None,\n",
    "            \"thumb_colorfulness\": None,\n",
    "        })).apply(pd.Series)\n",
    "\n",
    "        df = pd.concat([df.reset_index(drop=True), feats_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    if out_csv_path:\n",
    "        df.to_csv(out_csv_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example run\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_CSV = os.path.join(PROJECT_DIR, \"youtube_data.csv\")\n",
    "    OUTPUT_CSV = os.path.join(PROJECT_DIR, \"youtube_data_enriched.csv\")\n",
    "    THUMB_DIR = os.path.join(PROJECT_DIR, \"thumb_cache\")\n",
    "\n",
    "    df_enriched = enrich_youtube_csv(\n",
    "        csv_path=INPUT_CSV,\n",
    "        video_id_col=\"video_id\",\n",
    "        url_col=\"url\",\n",
    "        out_csv_path=OUTPUT_CSV,\n",
    "        add_thumbnail_features=True,\n",
    "        thumbnail_cache_dir=THUMB_DIR,\n",
    "    )\n",
    "\n",
    "    print(df_enriched.shape)\n",
    "    print(df_enriched.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8ae27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "430-0-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
